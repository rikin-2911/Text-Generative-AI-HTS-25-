{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text data.\n",
    "with open('english_train.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there \n"
     ]
    }
   ],
   "source": [
    "lines_of_text = 20000\n",
    "\n",
    "with open('english_train.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = [next(file) for _ in range(lines_of_text)]\n",
    "\n",
    "# Join lines into a single string\n",
    "text = ''.join(lines)\n",
    "print('Sample Text:', text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4521185"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pre-processing..\n",
    "import re\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove special characters and digits\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "text = re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the torchtext module\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Making a tokenizer for tokenization of all the words in a text.\n",
    "english_tokenizer = get_tokenizer('basic_english') # English word tokens.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the tokenizer to the text for making the tokens.\n",
    "tokens = english_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831137"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the max padding\n",
    "MAX_PADDING = 100\n",
    "\n",
    "# Making padding function\n",
    "def pad_token(tokens):\n",
    "  if(len(tokens)) >= MAX_PADDING:\n",
    "    return tokens[:MAX_PADDING]\n",
    "  else:\n",
    "    return tokens + [pad_token] * (MAX_PADDING - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the vocabulary.\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator # Vocab module\n",
    "\n",
    "# Defining the vocabulary size.\n",
    "VOCAB_SIZE = 100_000\n",
    "\n",
    "# Some Special Conditions.\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "# Vocabulary\n",
    "vocab = build_vocab_from_iterator([tokens], max_tokens=VOCAB_SIZE,\n",
    "                                  specials=[unk_token, pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16419"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is some basic implementation for handling the unknown tokens. By making the default index\n",
    "vocab.set_default_index(vocab[unk_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1811"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the token's indices..\n",
    "sample_view = vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 55, 7, 23, 50, 239, 57, 160, 53, 11487]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_view[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([831037, 100])\n",
      "Output Shape: torch.Size([831037])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "# Convert all tokens to indices using the vocabulary\n",
    "encoded_text = [vocab[token] for token in tokens]\n",
    "\n",
    "# Set the sequence length\n",
    "sequence_length = 100\n",
    "\n",
    "# Create input-output sequences\n",
    "input_sequences = []\n",
    "output_tokens = []\n",
    "\n",
    "for i in range(sequence_length, len(encoded_text)):\n",
    "    input_sequences.append(encoded_text[i-sequence_length:i])\n",
    "    output_tokens.append(encoded_text[i])\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(input_sequences)\n",
    "y = torch.tensor(output_tokens)\n",
    "\n",
    "print('Input Shape:', X.shape)   # (num_sequences, sequence_length)\n",
    "print('Output Shape:', y.shape)  # (num_sequences,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches: 51940\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create TensorDataset and DataLoader for the entire dataset\n",
    "full_dataset = TensorDataset(X, y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print('Number of Batches:', len(full_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextGenerationLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256, num_layers=2): # Specifying the dimensions of embedding, hidden layers.\n",
    "        super(TextGenerationLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # Predict the next token in the linear layer.\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden states (h0, c0) with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGenerationLSTM(\n",
      "  (embedding): Embedding(16419, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=16419, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's Instantiate the model..\n",
    "\n",
    "# Instantiating the model\n",
    "vocab_size = len(vocab)  # Vocabulary size from torchtext\n",
    "model = TextGenerationLSTM(vocab_size)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Using the cross-entropy loss.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Using the Adam Optimization method/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in full_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        hidden = model.init_hidden(inputs.size(0), device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(full_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def generate_text(model, start_text, vocab, tokenizer, max_length=100, temperature=1.0):\n",
    "\n",
    "    # Tokenize the start text\n",
    "    tokens = tokenizer(start_text)\n",
    "    input_seq = [vocab[token] for token in tokens]\n",
    "    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "    # Initialize hidden states\n",
    "    hidden = model.init_hidden(input_seq.size(0), device)\n",
    "\n",
    "    # Collect generated tokens\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "\n",
    "            # Apply temperature to logits\n",
    "            output = output / temperature\n",
    "            probs = F.softmax(output, dim=-1).squeeze()\n",
    "\n",
    "            # Sample the next token\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_token = vocab.lookup_token(next_token_id)\n",
    "\n",
    "            # Stop if end of sequence token is generated\n",
    "            if next_token == '<eos>':\n",
    "                break\n",
    "\n",
    "            # Add the token to the generated sequence\n",
    "            generated_tokens.append(next_token)\n",
    "\n",
    "            # Update input sequence\n",
    "            input_seq = torch.tensor([next_token_id]).unsqueeze(0).to(device)\n",
    "\n",
    "    # Join tokens to form the final text\n",
    "    generated_text = ' '.join(generated_tokens)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " what is your name ? and if they pay attention in conclusion the people kept during the community stable until this people stay comfortable up and that just government dont be great saying all the persons will not be good but that will be a good thing to have positive things is everyone point they have to learn those who they would like to never be active if youre bad attitude esteem and skill they mean everything the best they dont like it etc also believe that person always always always all wont make out of their body and tell fun they accomplish more and\n"
     ]
    }
   ],
   "source": [
    "# Define the seed text to start generating\n",
    "seed_text = \"What is your name ? \"\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(model, seed_text, vocab, english_tokenizer, max_length=100, temperature=1.0)\n",
    "\n",
    "# Display the generated text\n",
    "print(\"\\nGenerated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenai-englang.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'genai-englang.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
