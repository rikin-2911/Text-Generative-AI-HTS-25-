{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxsrzE066R7F"
      },
      "source": [
        "# Text Generation Model using Gen-AI Architecture.\n",
        "## Using Advanced version of RNN (Recurrent Neural Networks) that is LSTM (Long-Short Term Memory).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HH-V-AN7UVc"
      },
      "source": [
        "- Outline/Structure of workflow:-\n",
        "1. Download and Gather data.\n",
        "2. Prepare the data for training.\n",
        "3. Build a LSTM model\n",
        "4. Train and Evaluate the model.\n",
        "5. Generate Text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQqEsqX67VE1"
      },
      "source": [
        "### 1. We have collected the data from various sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbL_pK5_9xc3"
      },
      "source": [
        "### 2. Pre-Processing the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the file path and the word to remove\n",
        "file_path = 'dialogues_text.txt'\n",
        "word_to_remove = '__eou__'\n",
        "\n",
        "# Read the file content\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace the specific word (this is case-sensitive)\n",
        "updated_content = content.replace(word_to_remove, '')\n",
        "\n",
        "# Write the updated content back to the file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(updated_content)\n",
        "\n",
        "print(f\"The word '{word_to_remove}' has been removed from the file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hap33BISS7zg"
      },
      "outputs": [],
      "source": [
        "# Load the text data.\n",
        "with open('dialogues_text.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijw_uklbStQg",
        "outputId": "379df8d3-4616-4df9-bcde-2b9aad27dd3c"
      },
      "outputs": [],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-v_2NZWivuZ"
      },
      "outputs": [],
      "source": [
        "import torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_f-ppBQ97I5"
      },
      "source": [
        "- Some basic pre-processing using the regex module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxAlBUYT9ohI"
      },
      "outputs": [],
      "source": [
        "# Basic pre-processing..\n",
        "import re\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove special characters and digits\n",
        "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "# Replace multiple spaces with a single space\n",
        "text = re.sub(r'\\s+', ' ', text).strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPfj2niw-kzN"
      },
      "source": [
        "- Advanced pre-processing of the data using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prhi2df7-Z89"
      },
      "outputs": [],
      "source": [
        "# Importing the torchtext module\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Making a tokenizer for tokenization of all the words in a text.\n",
        "tokenizer = get_tokenizer('basic_english') # English word tokens.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0tBWO_gAWMg"
      },
      "outputs": [],
      "source": [
        "# Fitting the tokenizer to the text for making the tokens.\n",
        "tokens = tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnj30VrW_p1i"
      },
      "source": [
        "- Building the vocabulary for text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a-2nW56VHkj"
      },
      "outputs": [],
      "source": [
        "# Defining the max padding\n",
        "MAX_PADDING = 100\n",
        "\n",
        "# Making padding function\n",
        "def pad_token(tokens):\n",
        "  if(len(tokens)) >= MAX_PADDING:\n",
        "    return tokens[:MAX_PADDING]\n",
        "  else:\n",
        "    return tokens + [pad_token] * (MAX_PADDING - len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZGY2Wsy_l_y"
      },
      "outputs": [],
      "source": [
        "# Implementing the vocabulary.\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator # Vocab module\n",
        "\n",
        "# Defining the vocabulary size.\n",
        "VOCAB_SIZE = 50_000\n",
        "\n",
        "# Some Special Conditions.\n",
        "unk_token = \"<unk>\"\n",
        "pad_token = \"<pad>\"\n",
        "\n",
        "# Vocabulary\n",
        "vocab = build_vocab_from_iterator([tokens], max_tokens=VOCAB_SIZE,\n",
        "                                  specials=[unk_token, pad_token])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5CyQkT7UyDc",
        "outputId": "e5e134a9-eb65-4026-96b9-9bf207f33112"
      },
      "outputs": [],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PeIYQApGYu6",
        "outputId": "546f716b-6698-4024-852a-482bb9571a18"
      },
      "outputs": [],
      "source": [
        "vocab[unk_token]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf7vp2x6EgJ-"
      },
      "outputs": [],
      "source": [
        "# There is some basic implementation for handling the unknown tokens. By making the default index\n",
        "vocab.set_default_index(vocab[unk_token])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNYOPaYPGR0n",
        "outputId": "cd5a8346-f8f6-4d32-c4e4-d976568ba300"
      },
      "outputs": [],
      "source": [
        "vocab['rikin'] # Example.!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbSSvC_1Gcr9"
      },
      "outputs": [],
      "source": [
        "# Viewing the token's indices..\n",
        "sample_view = vocab.lookup_indices(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGihwO_OGuet",
        "outputId": "de723b03-e158-4c29-86ea-d4b19681544d"
      },
      "outputs": [],
      "source": [
        "sample_view[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hlPZCfXXW5"
      },
      "source": [
        "### Now we have to make the input and output sequences for our model..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWE50fypXmtP",
        "outputId": "6ed836dc-80f3-4345-e9df-4b863c4a327e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy\n",
        "\n",
        "# Convert all tokens to indices using the vocabulary\n",
        "encoded_text = [vocab[token] for token in tokens]\n",
        "\n",
        "# Set the sequence length\n",
        "sequence_length = 100\n",
        "\n",
        "# Create input-output sequences\n",
        "input_sequences = []\n",
        "output_tokens = []\n",
        "\n",
        "for i in range(sequence_length, len(encoded_text)):\n",
        "    input_sequences.append(encoded_text[i-sequence_length:i])\n",
        "    output_tokens.append(encoded_text[i])\n",
        "\n",
        "# Convert to tensors\n",
        "X = torch.tensor(input_sequences)\n",
        "y = torch.tensor(output_tokens)\n",
        "\n",
        "print('Input Shape:', X.shape)   # (num_sequences, sequence_length)\n",
        "print('Output Shape:', y.shape)  # (num_sequences,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_F3snStnyQa"
      },
      "source": [
        "- Making a TensorDataset and Dataloader.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PICLIuw1nV3S",
        "outputId": "1725846f-e6a4-47e2-ccf5-e8b2dd897111"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDataset and DataLoader for the entire dataset\n",
        "full_dataset = TensorDataset(X, y)\n",
        "full_loader = DataLoader(full_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "print('Number of Batches:', len(full_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1mlhFsbok0m"
      },
      "source": [
        "### Now we will make the custom class for LSTM Model.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBulfx92oWJP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextGenerationLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256, num_layers=2): # Specifying the dimensions of embedding, hidden layers.\n",
        "        super(TextGenerationLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # Predict the next token in the linear layer.\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        # Initialize hidden states (h0, c0) with zeros\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return (h0, c0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsZ6Ovbmp6a9",
        "outputId": "d6e5f12f-99ef-430e-d637-193767743579"
      },
      "outputs": [],
      "source": [
        "# Let's Instantiate the model..\n",
        "\n",
        "# Instantiating the model\n",
        "vocab_size = len(vocab)  # Vocabulary size from torchtext\n",
        "model = TextGenerationLSTM(vocab_size)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am8zAtI1qR-9"
      },
      "source": [
        "## Loss and Optimization of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIttgcVoqrZT"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Using the cross-entropy loss.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Using the Adam Optimization method/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxOp7Ditq2Wi",
        "outputId": "5abd24bb-17ea-4d96-ca6f-093fd0043b2c"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "num_epochs = 5\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, targets in full_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Initialize hidden states\n",
        "        hidden = model.init_hidden(inputs.size(0), device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(full_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB7eVSbi6XVY"
      },
      "source": [
        "## Finally implementing the generate function for generating text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-33SnkZ6fgj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def generate_text(model, start_text, vocab, tokenizer, max_length=100, temperature=1.0):\n",
        "\n",
        "    # Tokenize the start text\n",
        "    tokens = tokenizer(start_text)\n",
        "    input_seq = [vocab[token] for token in tokens]\n",
        "    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)  # (1, seq_len)\n",
        "\n",
        "    # Initialize hidden states\n",
        "    hidden = model.init_hidden(input_seq.size(0), device)\n",
        "\n",
        "    # Collect generated tokens\n",
        "    generated_tokens = tokens.copy()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "\n",
        "            # Apply temperature to logits\n",
        "            output = output / temperature\n",
        "            probs = F.softmax(output, dim=-1).squeeze()\n",
        "\n",
        "            # Sample the next token\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
        "            next_token = vocab.lookup_token(next_token_id)\n",
        "\n",
        "            # Stop if end of sequence token is generated\n",
        "            if next_token == '<eos>':\n",
        "                break\n",
        "\n",
        "            # Add the token to the generated sequence\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "            # Update input sequence\n",
        "            input_seq = torch.tensor([next_token_id]).unsqueeze(0).to(device)\n",
        "\n",
        "    # Join tokens to form the final text\n",
        "    generated_text = ' '.join(generated_tokens)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfF7ytcY7Lf0"
      },
      "source": [
        "# Use the function for generating text.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-HXCzxW7IE2",
        "outputId": "d08a3bb0-ba55-4b1b-d8b3-f723c9a4aad0"
      },
      "outputs": [],
      "source": [
        "# Define the seed text to start generating\n",
        "seed_text = \"May I help you find something , sir \"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, seed_text, vocab, tokenizer, max_length=25, temperature=0.99)\n",
        "\n",
        "# Display the generated text\n",
        "print(\"\\nGenerated Text:\\n\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "torch.save(model, 'genai-Q&A.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
