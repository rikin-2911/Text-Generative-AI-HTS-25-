{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Text Generation Model Documentation\n",
    "**Team name:** [MECHAMINDS]  \n",
    "**Date:** February 2025  \n",
    "**Version:** 3.0  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This project implements a **LSTM(LONG SHORT TERM MEMORY)** for text generation, trained using **Wikibooks, General Question Answering Datasets(kind~GPT), and Story Book Dataset**. The goal is to create human-like text generation that is **Indistinguishable from AI-generated content**.\n",
    "\n",
    "The model was built for the **[HTS'25 CODEVERSE]**, where the objective was to develop a generative AI solution **without using external APIs**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visual Studio Code Environment Setup.\n",
    "- Python 3.11.0 language (Suitable version for this project).\n",
    "- PyTorch 2.15.0 library.\n",
    "- Pandas library.\n",
    "- Scikit-Learn library.\n",
    "- Matplotlib library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- !git clone [repository_link]\n",
    "- !cd project_directory\n",
    "- !pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import torch\n",
    "- print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "This model is based on **LSTM(LONG SHORT TERM MEMORY)**. It was fine-tuned on **Wikibooks, General Question Answering Datasets(kind~GPT), and Story Book Dataset** to enhance natural text generation.\n",
    "\n",
    "### Features:\n",
    "âœ… Uses **LSTM(LONG SHORT TERM MEMORY)** for deep learning  \n",
    "âœ… Implements **PyTorch** for training  \n",
    "âœ… Supports **English text generation**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Process\n",
    "The model was trained using the following configuration:\n",
    "### 1. Download and Explore Datasets.\n",
    "- WIKIBOOKS/ E.A. POE'S CORPUS SHORTSTORIES DATASET. (Kaggle)\n",
    "- ENGLISH DIALOGUES DATASET. (From Web)\n",
    "- BASIC ENGLISH TEXT DATASET. (Kaggle)\n",
    "\n",
    "### 2. Preapare the data for training. USing NLP and TorchText.\n",
    "- Basic Pre-processing using regex (Regular Expression).\n",
    "- Implementing/Making the tokenizer for making tokens from the text.\n",
    "- Fitting the tokenizer to the text.\n",
    "- Implementing padding for the same number of inputs.\n",
    "- Making the vocabulary (Important Step.)\n",
    "- Indexing the vocabulary.\n",
    "- Making the input and output sequences and converting it into PyTorch Tensors.\n",
    "\n",
    "### 3. TensorDataset and DataLoader.\n",
    "- Creation of Pytorch Datasets and Pytorch DataLoader for Neural Networks.\n",
    "\n",
    "### 4. Model Creation (LSTM) using Custom Class.\n",
    "- Steps:\n",
    "1. Input DataLoader to the embedding layer (1oo Dimensions) (Embedding layer: Makes vectors of the individual words.)\n",
    "2. Outputs of embedding layer goes to input of LSTM Layers (2 layers).\n",
    "3. Outputs of LSTM layers goes to Linear layer for the outputs.\n",
    "4. Implementing the forward pass.\n",
    "5. Instantiation of the class with vocabulary size.\n",
    "\n",
    "### 5. Loss Function and Optimization.\n",
    "- Loss function : Cross Entropy Loss.\n",
    "- Optimizer : Adam.\n",
    "### 6. Training and Evaluation of the model.\n",
    "1. Train the model epoch-by-epoch.\n",
    "2. Backpropagation for decreasing loss.\n",
    "3. Calculating the average loss.\n",
    "\n",
    "### 7. Generating Text.\n",
    "- Generate text using user input and generate text function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Inference & Usage\n",
    "- Once trained, the model can generate human-like text given a prompt.\n",
    "\n",
    "### Usuage and Future Scope\n",
    "- Can be use as a content creation, language translation, and marketing.\n",
    "- Can be use as a code generation.\n",
    "- Can be use as a college chatbot on college website for FAQ's and solve Student queries by giving the data.\n",
    "- Can be use in a college virtual library collection for answering the book's contents.\n",
    "- Can be use as a summarization of given text input and many more...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Limitations & Future Improvements\n",
    "### Limitations:\n",
    "ðŸš« The model sometimes generates **inconsistent** text.  \n",
    "ðŸš« It requires a **high-end GPU** and large datasets for training which requires high calculations.\n",
    "\n",
    "### Future Enhancements:\n",
    "âœ… Fine-tuning on **domain-specific datasets**.  \n",
    "âœ… Implementing **reinforcement learning for better text coherence**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "This project successfully trains a **LSTM(LONG SHORT TERM MEMORY)**, capable of producing human-like text. Future work includes improving model coherence and fine-tuning on custom datasets.\n",
    "\n",
    "## 8. References\n",
    "\n",
    "{\n",
    "\"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\":\"Aurelien Geron.(2022).Powered  by JUPYTER.[https://oreilly.com/catalog/errata.csp?isbn=9781492032649]\",\n",
    "\n",
    "\"Kaggle Dataset\": \"Kaggle. (2025). OpenWebText Dataset. [https://www.kaggle.com/datasets]\"\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF (End-Of-File)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
