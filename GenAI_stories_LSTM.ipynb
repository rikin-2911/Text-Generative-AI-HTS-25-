{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: The ways of God in Nature, as in Providence, are not as our ways; nor are the models that we frame any way commensurate to the vastness, profundity, and unsearchableness of His works, which have a dep\n"
     ]
    }
   ],
   "source": [
    "lines_of_text = 70\n",
    "\n",
    "with open('story_data.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = [next(file) for _ in range(lines_of_text)]\n",
    "\n",
    "# Join lines into a single string\n",
    "text = ''.join(lines)\n",
    "print('Sample Text:', text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1914347"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic pre-processing\n",
    "import re\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove special characters and digits (optional)\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "text = re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\Python\\Lib\\asyncio\\base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\Python\\Lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\Python\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22012\\2605034295.py\", line 2, in <module>\n",
      "    from torchtext.data.utils import get_tokenizer\n",
      "  File \"d:\\Python\\Lib\\site-packages\\torchtext\\__init__.py\", line 3, in <module>\n",
      "    from torch.hub import _get_torch_home\n",
      "  File \"d:\\Python\\Lib\\site-packages\\torch\\__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"d:\\Python\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"d:\\Python\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"d:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"d:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "d:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 24551\n"
     ]
    }
   ],
   "source": [
    "# Advanced pre-processing and vectorisation\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Build the vocabulary\n",
    "def yield_tokens(text):\n",
    "    yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print('Vocabulary Size:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([329889, 50])\n",
      "Output Shape: torch.Size([329889])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Convert all tokens to indices using the vocabulary\n",
    "encoded_text = [vocab[token] for token in tokens]\n",
    "\n",
    "# Set the sequence length\n",
    "sequence_length = 50\n",
    "\n",
    "\n",
    "# Create input-output sequences\n",
    "input_sequences = []\n",
    "output_tokens = []\n",
    "\n",
    "for i in range(sequence_length, len(encoded_text)):\n",
    "    input_sequences.append(encoded_text[i-sequence_length:i])\n",
    "    output_tokens.append(encoded_text[i])\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(input_sequences)\n",
    "y = torch.tensor(output_tokens)\n",
    "\n",
    "print('Input Shape:', X.shape)   # (num_sequences, sequence_length)\n",
    "print('Output Shape:', y.shape)  # (num_sequences,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches: 5155\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create TensorDataset and DataLoader for the entire dataset\n",
    "full_dataset = TensorDataset(X, y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print('Number of Batches:', len(full_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGenerationLSTM(\n",
      "  (embedding): Embedding(24551, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=24551, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextGenerationLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256, num_layers=2):\n",
    "        super(TextGenerationLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # Predict the next token\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden states (h0, c0) with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "# Instantiate the model\n",
    "vocab_size = len(vocab)  # Vocabulary size from torchtext\n",
    "model = TextGenerationLSTM(vocab_size)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 7.7672\n",
      "Epoch [2/10], Loss: 7.5131\n",
      "Epoch [3/10], Loss: 7.8824\n",
      "Epoch [4/10], Loss: 7.4394\n",
      "Epoch [5/10], Loss: 7.1941\n",
      "Epoch [6/10], Loss: 7.0639\n",
      "Epoch [7/10], Loss: 7.0393\n",
      "Epoch [8/10], Loss: 6.9908\n",
      "Epoch [9/10], Loss: 6.9566\n",
      "Epoch [10/10], Loss: 6.9405\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in full_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        hidden = model.init_hidden(inputs.size(0), device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(full_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def generate_text(model, start_text, vocab, tokenizer, max_length=100, temperature=1.0):\n",
    "   \n",
    "    # Tokenize the start text\n",
    "    tokens = tokenizer(start_text)\n",
    "    input_seq = [vocab[token] for token in tokens]\n",
    "    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "    \n",
    "    # Initialize hidden states\n",
    "    hidden = model.init_hidden(input_seq.size(0), device)\n",
    "    \n",
    "    # Collect generated tokens\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            \n",
    "            # Apply temperature to logits\n",
    "            output = output / temperature\n",
    "            probs = F.softmax(output, dim=-1).squeeze()\n",
    "            \n",
    "            # Sample the next token\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_token = vocab.lookup_token(next_token_id)\n",
    "            \n",
    "            # Stop if end of sequence token is generated\n",
    "            if next_token == '<eos>':\n",
    "                break\n",
    "            \n",
    "            # Add the tokn to the generated sequence\n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            # Update input sequence\n",
    "            input_seq = torch.tensor([next_token_id]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Join tokens to formd the final text\n",
    "    generated_text = ' '.join(generated_tokens)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " true ! – nervous – very , very dreadfully nervous i had been and am but why will you say that i am mad ? of in uneasiness again would graves there will always finally waited why when assumed the more upon the the press he is shrouded engaged ten knew it effected lay of the natural very circumstances among pantaloons overboard the careless kisses perceive electrical were politics he had to no invention upon an firm have soon but on up and about which while he rode in its consequence it lie pretending just repeated mystified i picked afterwards did repeated frequently upon lucretius open our suit in the first decisionsthe have thought one still beyond good open a poor lips that had before skeered and in night now of armed ever absolutely visible de gaping among a tongues that was necessarily attempt person as which or color at my article contes all in example in the stomach it do to our bottom not diversity i had received it and will buried either gust was exceedingly characterized of her condition in a positive not now was i could believe as to rise of its recovering tell there strangely i shall have committed i was may roule i be was matured and imagines they so abnormal say it while when the rim is too i indeed some code of the favor lest mere isthe can too of so testimony were inspector uttered to show its gruff expresses the commentary of half better expresses far expresses nervous ever felt coincidences for number could have more entertained and circular sensation against the color of our sustain plumage\n"
     ]
    }
   ],
   "source": [
    "# Define the seed text to start generating\n",
    "seed_text = \"True! – nervous – very, very dreadfully nervous I had been and am; but why will you say that I am mad?\"\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(model, seed_text, vocab, tokenizer, max_length=250, temperature=1.1)\n",
    "\n",
    "# Display the generated text\n",
    "print(\"\\nGenerated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last step is of deploying the model to streamlit.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'genai-stories.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
